{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Supervised Learning on the labelled data</h1>\n",
    "<h3>Here, we will use the supervised classification technique for the binary classification of the image patches extracted using RST as CMB or Non-CMB. The classifier used here will be Support Vector Machine(SVM) with Radial Basis Function(RBF) kernel.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /Users/shwetankpanwar/Documents/BTPData/python_scripts/cmb_segmentation\n",
      "The file structure creation failed. Structure already exists!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pfca import init_path\n",
    "from pfca.visuals.img2D import imgplot\n",
    "from pfca.core.preprocessing import *\n",
    "from pfca.core.processing import *\n",
    "from pfca import file_read as f\n",
    "import ants\n",
    "import skimage.feature as feature\n",
    "from time import time\n",
    "from pfca.exp.results import roi_snipping\n",
    "from pfca.exp.dataset import *\n",
    "from pfca import init_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dir, nifti_dir = init_path()\n",
    "cur_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>After the initialization, we will import the data stored in the pickle files and merge them in order to increase our sample size</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataset(dataset):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    cur_path = os.getcwd()\n",
    "    ds = pd.read_pickle(cur_path + '/datasets/learning/' + dataset)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdataset = import_dataset('rst_output_all_subjects_2_2_0.1')          #Data from positive RST\n",
    "ndataset = import_dataset('rst_output_all_subjects_negative_2_2_0.1') #Data from negative RST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_data = pdataset.append(ndataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of labelled datapoints: 2599\n",
      "Total no of target datapoints: 123\n",
      "Total no of non-target datapoints: 2476\n"
     ]
    }
   ],
   "source": [
    "print(\"Total no of labelled datapoints: \" + str(len(labelled_data)))\n",
    "print(\"Total no of target datapoints: \" + str(len(labelled_data.loc[\n",
    "    labelled_data['label'] == 1\n",
    "])))\n",
    "print(\"Total no of non-target datapoints: \" + str(len(labelled_data.loc[\n",
    "    labelled_data['label'] == 0\n",
    "])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>First, we will try to evaluate the performance on the original non-augmented dataset and see how our classifier performs</h3>\n",
    "<p>The results will be used to see if the data augmentation improves performance or not.</p>\n",
    "<p>The evaluation will be done based on k-fold cross validation</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to perform k-fold splitting in the dataset\n",
    "def k_fold_split(dataset, k, balanced = True):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        k = dataset\n",
    "        targets = k.loc[k['label'] == 1]         #Target data\n",
    "        non_targets = k.loc[k['label'] == 0]     #Non target data\n",
    "\n",
    "        n_targets = len(targets)\n",
    "        n_non_t = len(non_targets)\n",
    "\n",
    "        n_comm = min(n_targets, n_non_t)\n",
    "        list_target = targets.index.values\n",
    "        list_non_targets = non_targets.index.values\n",
    "\n",
    "        if balanced == True:  # It will produce a balanced class with equal no of targets and non-targets\n",
    "            # Decategorizing data in case of appending\n",
    "            k['session'] = np.nan\n",
    "\n",
    "            num = int(ratio * n_comm)  # no of target points to select\n",
    "\n",
    "            trainees = np.random.choice(list_target, size=(num,), replace=False)\n",
    "            trainees2 = np.random.choice(list_non_targets, size=(num,), replace=False)\n",
    "            all_train = np.append(trainees, trainees2)\n",
    "            k.loc[all_train, ['session']] = 'train'  # Labelling the datapoints as training points\n",
    "\n",
    "            num_test = n_comm - num  # No of test target points\n",
    "            target_left = k.loc[(k['label'] == 1) & (k['session'] != 'train')]\n",
    "            non_targets_left = k.loc[(k['label'] == 0) & (k['session'] != 'train')]\n",
    "            list_target_left = target_left.index.values\n",
    "            list_non_targets_left = non_targets_left.index.values\n",
    "            tests1 = np.random.choice(list_target_left, size=(num_test,), replace=False)\n",
    "            tests2 = np.random.choice(list_non_targets_left, size=(num_test,), replace=False)\n",
    "            all_test = np.append(tests1, tests2)\n",
    "            k.loc[all_test, ['session']] = 'test'  # Labelling the datapoints as training points\n",
    "            return    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
